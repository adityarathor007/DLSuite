{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizer,CLIPTextModel\n",
    "import re\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import argparse\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from PIL import Image, ImageFont, ImageDraw, ImageOps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextConditioner(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.transformer=CLIPTextModel.from_pretrained('openai/clip-vit-large-patch14') #processes the tokenized text to extract embeddings\n",
    "        self.tokenizer=CLIPTokenizer.from_pretrained('openai/clip-vit-large-patch14') #convert raw text into tokens(mapping of words)\n",
    "\n",
    "        \n",
    "        self.transformer.eval()\n",
    "\n",
    "        for param in self.transformer.parameters():\n",
    "            param.requires_grad=False\n",
    "\n",
    "\n",
    "    def forward(self, prompt):\n",
    "        print(\"The prompt is: \",prompt)\n",
    "        batch_encoding = self.tokenizer(prompt, truncation=True, max_length=77, return_length=True, return_overflowing_tokens=False, padding=\"max_length\", return_tensors=\"pt\")\n",
    "        print(\"output from tokenizer:\",batch_encoding)\n",
    "        \n",
    "        words = self.tokenizer.convert_ids_to_tokens(batch_encoding['input_ids'].view(-1).tolist()) \n",
    "        print(\"Getting back the words from the token: \",words)\n",
    "\n",
    "        text_embedding = self.transformer(batch_encoding[\"input_ids\"].cuda()) #the input_ids are token given to each word\n",
    "        print(\"contextualized_text_embedding from transformer: \",text_embedding.last_hidden_state.shape) #each token has a 768 size 1d array embedding in the output \n",
    "        return text_embedding.last_hidden_state.cuda(), batch_encoding[\"attention_mask\"].cuda() # 1, 77, 768 and  1, 77\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "The prompt is:  Hello World\n",
      "output from tokenizer: {'input_ids': tensor([[49406,  3306,  1002, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407, 49407,\n",
      "         49407, 49407, 49407, 49407, 49407, 49407, 49407]]), 'attention_mask': tensor([[1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0]]), 'length': tensor([77])}\n",
      "Getting back the words from the token:  ['<|startoftext|>', 'hello</w>', 'world</w>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>', '<|endoftext|>']\n",
      "contextualized_text_embedding from transformer:  torch.Size([1, 77, 768])\n"
     ]
    }
   ],
   "source": [
    "text_encoder=TextConditioner().cuda().eval()\n",
    "\n",
    "prompt=\"Hello World\"\n",
    "print(len(prompt.split(' ')))\n",
    "\n",
    "text_embedding,mask=text_encoder(prompt)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
