{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Country  Population\n",
      "0   Brazil   5536847.2\n",
      "1    China  17815724.0\n",
      "2  Germany   1795776.6\n",
      "3    India   9170929.4\n",
      "4    Japan   4830745.4\n",
      "5       UK   2418711.6\n",
      "6      USA   3787755.2\n"
     ]
    }
   ],
   "source": [
    "# Read data from CSV file\n",
    "import pandas as pd\n",
    "population_data = pd.read_csv('https://raw.githubusercontent.com/anandmishra22/PRML-Spring-2023/main/programmingAssignment/PA1/DATA/population.csv')\n",
    "\n",
    "# Convert 'Population' column to numeric by removing all commas\n",
    "population_data['Population'] = population_data['Population'].replace(',', '',regex=True).astype(float)\n",
    "\n",
    "\n",
    "# Group by 'Country' and calculate the average population for each country's cities\n",
    "average_population = population_data.groupby('Country')['Population'].mean().reset_index()\n",
    "print(average_population)\n",
    "\n",
    "# # Rounding of the population as it can't be a decimal value\n",
    "# average_population['Population'] = average_population['Population'].round()\n",
    "\n",
    "# # adding commas in form of thousand seperators\n",
    "# average_population['Population'] = average_population['Population'].apply('{:,}'.format)\n",
    "\n",
    "# # printing the average population\n",
    "# print(average_population)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "In PyTorch, tensors are used to represent the inputs and outputs of a model, as well as the model's parameters.\n",
    "\n",
    "Similar to NumPyâ€™s ndarrays, tensors offer powerful multidimensional array capabilities, but with the added advantage of being able to run on GPUs and other hardware accelerators. Additionally, tensors and NumPy arrays can frequently share the same underlying memory, which removes the need for data duplication.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type of x_data: <class 'torch.Tensor'> \n",
      "\n",
      "Data type of np_array: <class 'numpy.ndarray'> \n",
      "\n",
      "Data type of x_np: <class 'torch.Tensor'> \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1, 1],\n",
      "        [1, 1]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.9841, 0.5495],\n",
      "        [0.2924, 0.8301]]) \n",
      "\n",
      "Random Tensor: \n",
      " tensor([[0.2538, 0.2664, 0.5072],\n",
      "        [0.5894, 0.0625, 0.1299]]) \n",
      "\n",
      "Ones Tensor: \n",
      " tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]]) \n",
      "\n",
      "Zeros Tensor: \n",
      " tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# Initialize tensor from list\n",
    "data=[[1,2],[3,4]]   \n",
    "x_data=torch.tensor(data)\n",
    "print(f\"Data type of x_data: {type(x_data)} \\n\")\n",
    "\n",
    "\n",
    "\n",
    "# Intialize tensor from a numpy array\n",
    "np_array=np.array(data)\n",
    "print(f\"Data type of np_array: {type(np_array)} \\n\")\n",
    "x_np=torch.from_numpy(np_array)\n",
    "print(f\"Data type of x_np: {type(x_np)} \\n\")\n",
    "\n",
    "\n",
    "# Intialize tensor from other tensor\n",
    "x_ones=torch.ones_like(x_data) #retains the properties of x_data\n",
    "print(f\"Ones Tensor: \\n {x_ones} \\n\")\n",
    "\n",
    "\n",
    "x_rand=torch.rand_like(x_data,dtype=torch.float) #overrides the datatype of x_data \n",
    "print(f\"Random Tensor: \\n {x_rand} \\n\")\n",
    "\n",
    "\n",
    "# Initialize tensor from dimesionsality input\n",
    "shape=(2,3,)\n",
    "rand_tensor=torch.rand(shape)\n",
    "ones_tensor=torch.ones(shape)\n",
    "zeros_tensor=torch.zeros(shape)\n",
    "\n",
    "print(f\"Random Tensor: \\n {rand_tensor} \\n\")\n",
    "print(f\"Ones Tensor: \\n {ones_tensor} \\n\")\n",
    "print(f\"Zeros Tensor: \\n {zeros_tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[[]] : number of inside '[]' decide the shape of the 2nd dimension and number values inside it decide the shape of the 1st dimension\n",
    "\n",
    "[[[]]]: number of 2d array decide the shape of 3rd dimension and inside 2nd dimension and 1st dimension similar to as above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing and Manipluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: torch.Size([3, 4])\n",
      "Datatype of tensor: torch.float32\n",
      "Device tensor is stored on: cpu\n",
      "First row: tensor([-1.0009, -0.5641,  0.7430, -1.6229])\n",
      "First column: tensor([-1.0009,  0.4289, -1.5366, -0.9612])\n",
      "Last column: tensor([-1.6229, -0.4759,  0.8513,  1.7465])\n",
      "tensor([[-1.0009,  0.0000,  0.7430, -1.6229],\n",
      "        [ 0.4289,  0.0000,  0.6219, -0.4759],\n",
      "        [-1.5366,  0.0000,  0.9039,  0.8513],\n",
      "        [-0.9612,  0.0000, -0.1983,  1.7465]])\n",
      "tensor([[-1.0009,  0.0000,  0.7430, -1.6229, -1.0009,  0.0000,  0.7430, -1.6229,\n",
      "         -1.0009,  0.0000,  0.7430, -1.6229],\n",
      "        [ 0.4289,  0.0000,  0.6219, -0.4759,  0.4289,  0.0000,  0.6219, -0.4759,\n",
      "          0.4289,  0.0000,  0.6219, -0.4759],\n",
      "        [-1.5366,  0.0000,  0.9039,  0.8513, -1.5366,  0.0000,  0.9039,  0.8513,\n",
      "         -1.5366,  0.0000,  0.9039,  0.8513],\n",
      "        [-0.9612,  0.0000, -0.1983,  1.7465, -0.9612,  0.0000, -0.1983,  1.7465,\n",
      "         -0.9612,  0.0000, -0.1983,  1.7465]])\n"
     ]
    }
   ],
   "source": [
    "#attributes of tensor\n",
    "tensor=torch.rand(3,4)\n",
    "\n",
    "print(f\"Shape of tensor: {tensor.shape}\")\n",
    "print(f\"Datatype of tensor: {tensor.dtype}\")\n",
    "print(f\"Device tensor is stored on: {tensor.device}\")\n",
    "\n",
    "\n",
    "\n",
    "#Indexing and slicing a tensor\n",
    "tensor=torch.randn(4,4)\n",
    "print(f\"First row: {tensor[0]}\")\n",
    "print(f\"First column: {tensor[:,0]}\")\n",
    "print(f\"Last column: {tensor[...,-1]}\")\n",
    "tensor[:,1]=0\n",
    "print(tensor)\n",
    "\n",
    "\n",
    "# Joining two tensor\n",
    "t1=torch.cat([tensor,tensor,tensor],dim=1)\n",
    "print(t1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear vs Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "X_train=np.array([[1,0,1,0],[0,0,1,1], [1,1,1,0]])\n",
    "\n",
    "row=X_train[0]\n",
    "print(row.shape)\n",
    "\n",
    "w_linear=nn.Linear(4,3,bias=False)\n",
    "print(w_linear.weight)\n",
    "\n",
    "print(w_linear(torch.FloatTensor(row)))\n",
    "\n",
    "w_embedding=nn.Embedding(4,3).from_pretrained(w_linear.weight.T)\n",
    "print(w_embedding.weight)\n",
    "\n",
    "sparse_row=torch.tensor(row,dtype=torch.long)\n",
    "\n",
    "non_zero_indicies=torch.nonzero(sparse_row).squeeze(1)\n",
    "print(non_zero_indicies)\n",
    "\n",
    "print(w_embedding(non_zero_indicies))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unsqueeze and Squeeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3]])\n",
      "torch.Size([1, 3])\n"
     ]
    }
   ],
   "source": [
    "tensor=torch.tensor([1,2,3]) #Shpae [3]\n",
    "unsqueezed_tensor=tensor.unsqueeze(0)  #adding dimension at index 0\n",
    "print(unsqueezed_tensor)\n",
    "print(unsqueezed_tensor.shape)\n",
    "\n",
    "\n",
    "\n",
    "# squeeze used to remove those dimension which have shape of 1\n",
    "tensor=torch.rand(1,3,256,256)\n",
    "\n",
    "tensor.squeeze(0)   #only removes if dimension specified having shape as 1 else return as it is(very important)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5445, 0.0376, 0.6943],\n",
      "        [0.7321, 0.2462, 0.0233]])\n",
      "tensor([[0.5445, 0.7321],\n",
      "        [0.0376, 0.2462],\n",
      "        [0.6943, 0.0233]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "shape=(2,3,)\n",
    "t=torch.rand(shape)\n",
    "print(t)\n",
    "\n",
    "print(t.transpose(-2,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View\n",
    "returns a new tensor with the same data as the self tensor but of different shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data's stride (24, 12, 4, 1)\n",
      "stride after view (24, 12, 3, 1)\n",
      "tensor([[ 1.1863, -1.0208, -0.3183],\n",
      "        [-0.0614, -0.6214,  0.2219]])\n",
      "tensor([ 1.1863, -1.0208, -0.3183, -0.0614, -0.6214,  0.2219])\n",
      "tensor([[-0.3429,  0.8406,  0.7283, -0.7830],\n",
      "        [ 0.0914, -0.8639, -1.7109, -1.3927],\n",
      "        [ 0.5615,  1.3659, -0.8780, -0.0956],\n",
      "        [ 1.8997, -0.2655, -0.8522,  2.1720]])\n",
      "tensor([[-0.3429,  0.8406,  0.7283, -0.7830,  0.0914, -0.8639, -1.7109, -1.3927],\n",
      "        [ 0.5615,  1.3659, -0.8780, -0.0956,  1.8997, -0.2655, -0.8522,  2.1720]])\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(1,2,3,4)\n",
    "print(\"Original data's stride\",a.stride())\n",
    "c=a.view(1,2,4,3)   #no change in the stride\n",
    "print(\"stride after view\",c.stride())\n",
    "\n",
    "test=torch.randn(2,3)\n",
    "print(test)\n",
    "print(test.view(-1))\n",
    "\n",
    "\n",
    "t1=torch.randn(4,4)\n",
    "z=t1.view(-1,8) #the size -1 is infered from other dimension\n",
    "print(t1)\n",
    "print(z)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 3, 4])\n",
      "tensor([[[[-1.4143, -0.1577, -0.1069, -1.0023],\n",
      "          [ 1.0648, -0.6918, -0.0435,  0.6764],\n",
      "          [ 1.0746,  0.1258, -0.1263, -0.9120]],\n",
      "\n",
      "         [[ 0.4684, -2.3826, -1.5607,  1.5103],\n",
      "          [ 0.3578, -0.6260,  1.2659,  0.6924],\n",
      "          [-1.0835,  1.6167, -0.0412,  0.5165]]]])\n",
      "Original data's stride (24, 12, 4, 1)\n",
      "stride after transpose (24, 12, 1, 4)\n",
      "torch.Size([1, 2, 4, 3])\n",
      "tensor([[[[-1.4143,  1.0648,  1.0746],\n",
      "          [-0.1577, -0.6918,  0.1258],\n",
      "          [-0.1069, -0.0435, -0.1263],\n",
      "          [-1.0023,  0.6764, -0.9120]],\n",
      "\n",
      "         [[ 0.4684,  0.3578, -1.0835],\n",
      "          [-2.3826, -0.6260,  1.6167],\n",
      "          [-1.5607,  1.2659, -0.0412],\n",
      "          [ 1.5103,  0.6924,  0.5165]]]])\n",
      "stride after view (24, 12, 3, 1)\n",
      "torch.Size([1, 2, 4, 3])\n",
      "tensor([[[[-1.4143, -0.1577, -0.1069],\n",
      "          [-1.0023,  1.0648, -0.6918],\n",
      "          [-0.0435,  0.6764,  1.0746],\n",
      "          [ 0.1258, -0.1263, -0.9120]],\n",
      "\n",
      "         [[ 0.4684, -2.3826, -1.5607],\n",
      "          [ 1.5103,  0.3578, -0.6260],\n",
      "          [ 1.2659,  0.6924, -1.0835],\n",
      "          [ 1.6167, -0.0412,  0.5165]]]])\n",
      "False\n",
      "True\n",
      "tensor([-1.4143, -0.1577, -0.1069, -1.0023,  1.0648, -0.6918, -0.0435,  0.6764,\n",
      "         1.0746,  0.1258, -0.1263, -0.9120,  0.4684, -2.3826, -1.5607,  1.5103,\n",
      "         0.3578, -0.6260,  1.2659,  0.6924, -1.0835,  1.6167, -0.0412,  0.5165])\n",
      "tensor([-1.4143,  1.0648,  1.0746, -0.1577, -0.6918,  0.1258, -0.1069, -0.0435,\n",
      "        -0.1263, -1.0023,  0.6764, -0.9120,  0.4684,  0.3578, -1.0835, -2.3826,\n",
      "        -0.6260,  1.6167, -1.5607,  1.2659, -0.0412,  1.5103,  0.6924,  0.5165])\n",
      "tensor([-1.4143, -0.1577, -0.1069, -1.0023,  1.0648, -0.6918, -0.0435,  0.6764,\n",
      "         1.0746,  0.1258, -0.1263, -0.9120,  0.4684, -2.3826, -1.5607,  1.5103,\n",
      "         0.3578, -0.6260,  1.2659,  0.6924, -1.0835,  1.6167, -0.0412,  0.5165])\n"
     ]
    }
   ],
   "source": [
    "a=torch.randn(1,2,3,4)\n",
    "print(a.size())\n",
    "print(a)\n",
    "\n",
    "\n",
    "b=a.transpose(-2,-1)  #actual mathematical transpose\n",
    "\n",
    "\n",
    "print(b.size())\n",
    "\n",
    "print(b)\n",
    "\n",
    "c=a.view(1,2,4,3)  # goes on assigning elment in the contigous order of memory\n",
    "\n",
    "print(c.size())  \n",
    "\n",
    "\n",
    "print(c)\n",
    "\n",
    "torch.equal(b,c)\n",
    "\n",
    "\n",
    "# #contiguous tensor have a straightforward mapping between its shape and how its data is stored in memory, whereas in non contiguous \n",
    "# reorder dimension without physically rearranging the data\n",
    "\n",
    "print(b.is_contiguous())  \n",
    "print(c.is_contiguous())\n",
    "\n",
    "# flattening(converting nd to 1d) of a and c is same and also same with respect to the contigous manner in which they were stored\n",
    "# whereas in b the on flattening since its the actual tranpose so elements no longer remain in the contigous manner in which it was present\n",
    "print(a.flatten())\n",
    "print(b.flatten())\n",
    "print(c.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clamp\n",
    "Clamps all elements in input into the range [ min, max ]. Letting min_value and max_value be min and max, respectively, this returns:\n",
    "\n",
    "yi=min(max(xi,min_valuei),max_valuei)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Repeat\n",
    "\n",
    "- repeat the tensor along the specified dimension\n",
    "- like .repeat(2,3) so repeats 2 times along the 2nd dimension and 3 times along the first dimension so if the shape is (3,2,4) then it will become (3,4,12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4])\n",
      "tensor([[1, 2, 3, 4],\n",
      "        [1, 2, 3, 4],\n",
      "        [1, 2, 3, 4]])\n",
      "torch.Size([3, 4])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "x=torch.tensor([1,2,3,4])\n",
    "# y=x.repeat(3,1,2) #means along the third dimension repeat 3 and along 1st dimension repeat once\n",
    "y=x.repeat(3,1)\n",
    "\n",
    "print(x.shape)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
