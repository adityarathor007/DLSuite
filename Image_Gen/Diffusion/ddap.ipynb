{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Denoising Diffusion Probablistic Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=[\"cuda:0\" if torch.cuda.is_available() else \"cpu\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearNoiseSchedular:\n",
    "    \n",
    "    def __init__(self,num_timesteps,beta_start,beta_end):\n",
    "        self.num_timesteps=num_timesteps\n",
    "        self.beta_start=beta_start\n",
    "        self.beta_end=beta_end\n",
    "        \n",
    "        self.betas=torch.linspace(beta_start,beta_end,num_timesteps)\n",
    "        self.alphas=1-self.betas\n",
    "        self.alpha_cum_prod=torch.cumprod(self.alphas,dim=0)\n",
    "        self.sqrt_alpha_cum_prod=torch.sqrt(self.alpha_cum_prod)\n",
    "        self.sqrt_one_minus_alpha_cum_prod=torch.sqrt(1-self.alpha_cum_prod)\n",
    "\n",
    "    \n",
    "    def add_noise(self,original,noise,t):\n",
    "        original_shape=original.shape\n",
    "        batch_size=original_shape[0]\n",
    "\n",
    "        sqrt_alpha_cum_prod=self.alpha_cum_prod[t].reshape(batch_size)\n",
    "        sqrt_one_minus_alpha_cum_prod=self.sqrt_one_minus_alpha_cum_prod[t].reshape(batch_size)\n",
    "\n",
    "        for _ in range(len(original_shape)-1):\n",
    "            sqrt_alpha_cum_prod=sqrt_alpha_cum_prod.unsqueeze(-1)  #[batch_size]->[batch_size,1]\n",
    "            sqrt_one_minus_alpha_cum_prod=sqrt_one_minus_alpha_cum_prod.unsqueeze(-1)\n",
    "\n",
    "\n",
    "        return sqrt_alpha_cum_prod*original+sqrt_one_minus_alpha_cum_prod*noise\n",
    "    \n",
    "\n",
    "    def sample_prev_timestep(self,xt,noise_pred,t):\n",
    "        x0=(xt-(self.sqrt_one_minus_alpha_cum_prod*noise_pred))/self.sqrt_alpha_cum_prod\n",
    "\n",
    "        x0=torch.clamp(x0,-1,1)\n",
    "\n",
    "        mean=xt-((self.betas[t]*noise_pred)/(self.sqrt_one_minus_alpha_cum_prod))\n",
    "        mean=mean/torch.sqrt(self.alphas[t])\n",
    "\n",
    "        if t==0:\n",
    "            return mean,x0\n",
    "        else:\n",
    "            variance=(1-self.alphas[t])*(1-self.alpha_cum_prod[t-1])\n",
    "            variance=variance/1-self.alpha_cum_prod[t]\n",
    "\n",
    "            sigma=variance**0.5\n",
    "\n",
    "            z=torch.randn(xt.shape).to(xt.device)\n",
    "\n",
    "            return mean+sigma*z,x0\n",
    "        \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m     t_emb\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcat([torch\u001b[38;5;241m.\u001b[39msin(t_emb),torch\u001b[38;5;241m.\u001b[39mcos(t_emb)],dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t_emb\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mDownBlock\u001b[39;00m(\u001b[43mnn\u001b[49m\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m,in_channels,out_channels,t_emb_dim,down_sample,num_heads):\n\u001b[1;32m     17\u001b[0m         \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
     ]
    }
   ],
   "source": [
    "# we also give the timestep we are at along with the image \n",
    "\n",
    "def get_time_embedding(time_steps,t_emb_dim):\n",
    "    factor=1000**((torch.arange(\n",
    "        start=0,end=t_emb_dim//2,device=time_steps.device) / (t_emb_dim // 2)\n",
    "        ))\n",
    "    \n",
    "    t_emd=time_steps[:, None].repeat(1,t_emb_dim//2) /factor\n",
    "\n",
    "    t_emb=torch.cat([torch.sin(t_emb),torch.cos(t_emb)],dim=-1)\n",
    "\n",
    "    return t_emb\n",
    "\n",
    "\n",
    "class DownBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,t_emb_dim,down_sample,num_heads):\n",
    "        super().__init__()\n",
    "        self.resnet_conv_first=nn.Sequential(\n",
    "            nn.GroupNorm(8,in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=1,padding=1)\n",
    "        )\n",
    "        self.t_emb_layers=nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(t_emb_dim,out_channels)\n",
    "        )\n",
    "\n",
    "        self.resenet_conv_second=nn.Sequential(\n",
    "            nn.GroupNorm(8,out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.attention_norm=nn.GroupNorm(8,out_channels)\n",
    "        self.attention=nn.MultiHeadAttention(out_channels,num_heads,batch_first=True)\n",
    "        \n",
    "        self.residual_input_conv=nn.Conv2d(in_channels,out_channels,kernel_size=2)\n",
    "\n",
    "        self.down_sample_conv=nn.Conv2d(out_channels,out_channels,kernel_size=4,stride=2,padding=1) if self.down_sample else nn.Identity()\n",
    "\n",
    "    \n",
    "    def forward(self,x,t_emb):\n",
    "        out=x\n",
    "\n",
    "        # Resnet block \n",
    "        resnet_input=out\n",
    "        out=self.resnet_conv_first(out)\n",
    "        out=out+self.t_emb_layers(t_emb)[:,:,None,None]\n",
    "        out=self.resenet_conv_second(out)\n",
    "        out=out+self.residual_input_conv(resnet_input)\n",
    "\n",
    "        # Attention Block\n",
    "        batch_size,channels,h,w=out.shape\n",
    "        in_attn=out.reshape(batch_size,channels,h*w)\n",
    "        in_attn=self.attention_norm(in_attn)\n",
    "        in_attn=in_attn.transpose(1,2) # to ensure the channels features are the last features\n",
    "\n",
    "\n",
    "        out_attn,_=self.attention(in_attn,in_attn,in_attn)\n",
    "        out_attn=out_attn.transpose(1,2).reshape(batch_size,channels,h,w)\n",
    "        out=out+out_attn\n",
    "\n",
    "\n",
    "        out=self.down_sample_conv(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MidBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,t_emb_dim,num_heads):\n",
    "        super().__init__()\n",
    "        self.resnet_conv_first=nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8,in_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=1,paddding=1)\n",
    "\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8,out_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n",
    "\n",
    "            )\n",
    "        ])\n",
    "\n",
    "        self.t_emb_layers=nn.ModuleList(\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim,out_channels)\n",
    "\n",
    "            ),\n",
    "            nn.Sequential(\n",
    "                nn.SiLU(),\n",
    "                nn.Linear(t_emb_dim,out_channels)\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.resnet_conv_second=nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.GroupNorm(8,out_channels),\n",
    "                nn.SiLU(),\n",
    "                nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n",
    "            )\n",
    "        ])\n",
    "\n",
    "\n",
    "        self.attention_norm=nn.GroupNorm(8,out_channels)\n",
    "        self.attention=nn.MultiheadAttention(out_channels,num_heads,batch_first=True)\n",
    "\n",
    "\n",
    "        self.residual_input_conv=nn.ModuleList([\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=1),\n",
    "            nn.Conv2d(out_channels,out_channels,kernal_size=1)\n",
    "        ])\n",
    "\n",
    "    def forward(self,x,t_emb):\n",
    "        out=x\n",
    "        # first resnet block \n",
    "        resnet_input=out\n",
    "        out=self.resnet_conv_first[0](out)\n",
    "        out=out+self.t_emb_layers[0](t_emb)[:,:,None,None]\n",
    "        out=self.resnet_conv_second[0](out)\n",
    "\n",
    "        out=out+self.residual_input_conv[0](resnet_input)\n",
    "\n",
    "        # attention block\n",
    "\n",
    "        batch_size,channels,h,w=out.shape\n",
    "        in_attn=out.reshape(batch_size,channels,h*w)\n",
    "        in_attn=self.attention_norm(in_attn)\n",
    "        in_attn=in_attn.transpose(1,2)\n",
    "        out_attn=out_attn.tranpose(1,2).reshape(batch_size,channels,h,w)\n",
    "        out=out+out_attn\n",
    "\n",
    "        # second resnet block\n",
    "        resnet_input=out\n",
    "        out=self.resnet_conv_first[1](out)\n",
    "        out=out+self.t_emb_layers[1](t_emb)[:,:,None,None]\n",
    "        out=out+self.residual_input_conv[1](resnet_input)\n",
    "\n",
    "\n",
    "        return out\n",
    "    \n",
    "\n",
    "\n",
    "class UpBlock(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,t_emb_dim,up_sample,num_heads):\n",
    "        super().__init__()\n",
    "        self.up_sample=up_sample\n",
    "        self.resnet_conv_first=nn.Sequential(\n",
    "            nn.GroupNorm(8,in_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(in_channels,out_channels,kernel_size=3,stride=1,padding=1)\n",
    "        )\n",
    "\n",
    "        self.t_emb_layers=nn.Sequential(\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(t_emb_dim,out_channels)\n",
    "        )\n",
    "\n",
    "        self.resnet_conv_second=nn.Sequential(\n",
    "            nn.GroupNorm(8,out_channels),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(out_channels,out_channels,kernel_size=3,stride=1,padding=1)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.attention_norm=nn.GroupNorm(8,out_channels)\n",
    "        self.attention=nn.MultiheadAttention(out_channels,num_heads,batch_first=True)\n",
    "        self.residual_input_conv=nn.Conv2d(in_channels,out_channels,kernel_size=1)\n",
    "\n",
    "        self.up_sample_conv=nn.ConvTranspose2d(in_channels//2,in_channels//2,kernel_size=4,stride=2,padding=1) if self.up_sample else nn.Identity()\n",
    "\n",
    "    def forward(self,x,out_down,t_emb):\n",
    "        \n",
    "        x=self.up_sample_conv(x)\n",
    "        x=torch.cat([x,out_down],dim=1)\n",
    "\n",
    "        # Resnet_block\n",
    "        out=x\n",
    "        resnet_input=out\n",
    "        out=self.resnet_conv_first(out)\n",
    "        out=out+self.t_emb_layers(t_emb)[:,:,None,None]\n",
    "        out=self.resnet_conv_second(out)\n",
    "        out=out+self.residual_input_conv(resnet_input)\n",
    "\n",
    "        # Attention_Block   \n",
    "        batch_size,channels,h,w=out.shape\n",
    "        in_attn=out.reshape(batch_size,channels,h*w)\n",
    "        in_attn=self.attention_norm(in_attn)\n",
    "\n",
    "        in_attn=in_attn.tranpose(1,2)\n",
    "        out_attn,_=self.attention(in_attn,in_attn,in_attn)\n",
    "        out_attn=out_attn.transpose(1,2).reshape(batch_size,channels,h,w)\n",
    "        out=out+out_attn\n",
    "\n",
    "        return out\n",
    "        \n",
    "        \n",
    "\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self,im_channels):\n",
    "        super().__init__()\n",
    "        self.down_channels=[32,64,128,256]\n",
    "        self.mid_channels=[256,256,128]\n",
    "        # downsample argument\n",
    "        self.t_emb_dim=128\n",
    "        self.down_sample=[True,True,False]\n",
    "\n",
    "\n",
    "        # Time Embedding block had position embedding followed by linear layer with activation in between (this is different from the timestep layers which we had for each resent block this can only be called once in an entire forward pass at start to get the intial time step represetation)\n",
    "\n",
    "        self.t_proj=nn.Sequential(\n",
    "            nn.Linear(self.t_emb_dim,self.t_emb_dim),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(self.t_emb_dim,self.t_emb_dim)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.up_sample=list(reversed(self.down_sample))\n",
    "        self.conv_in=nn.Conv2d(im_channels,self.down_channels[0],kernel_size=3,padding=1)\n",
    "\n",
    "\n",
    "        self.downs=nn.ModuleList([])\n",
    "        for i in range(len(self.down_channels)-1):\n",
    "            self.downs.append(DownBlock(self.down_channels[i],self.down_channels[i+1],self.t_emb_dim,down_sample=self.down_sample[i],num_heads=4))\n",
    "        \n",
    "        self.mids=nn.ModuleList([])\n",
    "        for i in range(len(self.mid_channels-1)):\n",
    "            self.mids.append(MidBlock(self.mid_channels[i],self.mid_channels[i+1],self.t_emb_dim,num_heads=4))\n",
    "        \n",
    "\n",
    "        self.ups=nn.ModuleList([])\n",
    "\n",
    "        for i in reversed(range(len(self.down_channels-1))):\n",
    "            self.ups.append(UpBlock(self.down_channels[i]*2,self.down_channels[i-1] if i!=0 else 16, self.t_emb_dim,up_sample=self.down_sample[i],num_heads=4))\n",
    "\n",
    "\n",
    "\n",
    "        self.norm_out=nn.GroupNorm(8,16)\n",
    "        self.conv_out=nn.Conv2d(16,im_channels,kernel_size=3,padding=1)\n",
    "\n",
    "\n",
    "    def forward(self,x,t):\n",
    "        out=self.conv_in(x)\n",
    "        t_emb=get_time_embedding(t,self.t_emb_dim)\n",
    "        t_emb=self.t_proj(t_emb)\n",
    "\n",
    "        down_outs=[]\n",
    "        for down in self.downs:\n",
    "            print(out.shape)\n",
    "            down_outs.append(out)\n",
    "            out=down(out,t_emb)\n",
    "\n",
    "\n",
    "        for mid in self.mids:\n",
    "            print(out.shape)\n",
    "            out=mid(out,t_emb)\n",
    "\n",
    "        for up in self.ups:\n",
    "            down_out=down_outs.pop()\n",
    "            print(out,down_outs.shape)\n",
    "            out=up(out,down_out,t_emb)\n",
    "        \n",
    "        out=self.norm_out(out)\n",
    "        out=nn.SiLU()(out)\n",
    "        out=self.conv_out(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST_Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import tqdm\n",
    "import glob\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    \"\"\" created datatset class rather than using torchvision to allow replacement with other image dataset\"\"\"\n",
    "\n",
    "    def __init__(self,split,im_path,im_ext='png'):\n",
    "\n",
    "\n",
    "        self.split=split\n",
    "        self.im_ext=im_ext\n",
    "        self.images,self.labels=self.load_images(im_path)\n",
    "\n",
    "    def load_images(self,im_path):\n",
    "        \"\"\"Gets all images from the path specified and stacks them all up\"\"\"\n",
    "\n",
    "        assert os.path.exists(im_path),\"images path {} does not exists\".format(im_path)\n",
    "        ims=[]\n",
    "        labels=[]\n",
    "        for d_name in tqdm(os.listdir(im_path)):\n",
    "            for fname in glob.glob(os.path.join(im_path,d_name,'*.{}'.format(self.im_ext))):\n",
    "                ims.append(fname)\n",
    "                labels.append(int(d_name))\n",
    "\n",
    "        \n",
    "        print('Found {} images for split {}'.format(len(ims),self.split))\n",
    "\n",
    "        return ims,labels\n",
    "    \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        im=Image.open(self.images[index])\n",
    "        im_tensor=torchvision.transforms.ToTensor()(im)\n",
    "\n",
    "        # Convert input to -1 to 1 range\n",
    "        im_tensor=(2*im_tensor)-1\n",
    "        return im_tensor\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import yaml\n",
    "import numpy as np\n",
    "from  torch.optim import Adam\n",
    "\n",
    "\n",
    "def train(args):\n",
    "\n",
    "    # Read the config_file #\n",
    "    with open(args.config_path,'r') as file:\n",
    "        try:\n",
    "            config=yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "    \n",
    "    print(config)\n",
    "\n",
    "    #######\n",
    "\n",
    "\n",
    "    diffusion_config=config['diffusion_params']\n",
    "    dataset_config=config['dataset_params']\n",
    "    model_config=config['model_params']\n",
    "    train_config=config['train_params']\n",
    "\n",
    "\n",
    "\n",
    "    # Create a noise schedular\n",
    "\n",
    "    schedular=LinearNoiseSchedular(num_timesteps=diffusion_config['num_timesteps'],\n",
    "                                   beta_start=diffusion_config['beta_start'],\n",
    "                                   beta_end=diffusion_config['beta_end'])\n",
    "    \n",
    "    # Create the dataset\n",
    "    mnist=MnistDataset(\"train\",im_path=dataset_config['im_path'])  \n",
    "    mnist_loader=DataLoader(mnist,batch_size=train_config['batch_size'],shuffle=True,num_workers=4)\n",
    "\n",
    "    device=[\"cuda:0\" if torch.cuda.is_available() else \"cpu\"]\n",
    "\n",
    "    # Instantiate the model\n",
    "    model=UNet(model_config).to(device)\n",
    "    model.train()\n",
    "\n",
    "    # Create output directory\n",
    "    if not os.path.exists(train_config['task_name']):\n",
    "        os.mkdir(train_config['task_name'])\n",
    "\n",
    "    \n",
    "    # Load checkpoint if found\n",
    "    if os.path.exists(os.path.join(train_config['task_name'],train_config['ckpt_name'])):\n",
    "        print('Loading checkpoint as found one')\n",
    "        model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n",
    "                                                      train_config['ckpt_name']),map_location=device))\n",
    "        \n",
    "    \n",
    "    # Specify training parameters  \n",
    "    num_epochs=train_config['num_epochs']\n",
    "    optimizer=Adam(model.parameter(),lr=train_config['lr'])\n",
    "    criterion=torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "    # Run training\n",
    "    for epoch_idx in range(num_epochs):\n",
    "        losses=[]\n",
    "        for im in tqdm(mnist_loader):\n",
    "            optimizer.zero_grad()\n",
    "            im=im.float().to(device)\n",
    "\n",
    "            # Sample random noise\n",
    "            noise=torch.randn_like(im).to(device)\n",
    "\n",
    "            # sample timestep\n",
    "            t=torch.randint(0,diffusion_config['num_timestep'],(im.shape[0],)).to(device)\n",
    "\n",
    "            # add noise to images according to timestep\n",
    "            noisy_im=schedular.add_noise(im,noise,t)\n",
    "            noise_pred=model(noisy_im,t)\n",
    "\n",
    "            loss=criterion(noise_pred,noise)\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print('Finished epoch:{} | Loss: {:.4f}'.format(\n",
    "            epoch_idx+1,\n",
    "            np.mean(losses)\n",
    "        ))\n",
    "\n",
    "        torch.save(model.state_dict(),os.path.join(train_config['task_name'],train_config['ckpt_name']))\n",
    "        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argParse\n",
    "\n",
    "def infer(args):\n",
    "    with open(args.config_path,'r') as file:\n",
    "        try:\n",
    "            config=yaml.safe_load(file)\n",
    "        except yaml.YAMLError as exc:\n",
    "            print(exc)\n",
    "        \n",
    "    \n",
    "    print(config)\n",
    "\n",
    "\n",
    "    #####################\n",
    "\n",
    "    diffusion_config=config['diffusion_params'] \n",
    "    model_config=config['model_params']\n",
    "    train_config=config[\"train_params\"]\n",
    "\n",
    "\n",
    "    # load model with checkpoints\n",
    "    model=UNet(model_config).to(device)\n",
    "    model.load_state_dict(torch.load(os.path.join(train_config['task_name'],\n",
    "                                                  train_config['ckpt_name']),map_location=device))\n",
    "    \n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    # Create the noise scheduler\n",
    "    scheduler=LinearNoiseSchedular(num_timesteps=diffusion_config['num_timesteps'],beta_start=diffusion_config['beta_start'],beta_end=diffusion_config['beta_end'])\n",
    "\n",
    "    with torch.no_grad():\n",
    "        sample(model,scheduler,train_config,model_config,diffusion_config)\n",
    "\n",
    "\n",
    "def sample(model,scheduler,train_config,model_config,diffusion_config):\n",
    "\n",
    "    # Samples stepwise by going backward one timestep at a time\n",
    "\n",
    "    xt=torch.randn((train_config['num_samples'],model_config['im_channels'],model_config['im_size'],model_config['im_size'])).to(device)\n",
    "\n",
    "\n",
    "    for i in tqdm(reversed(range(diffusion_config['num_timesteps']))):\n",
    "        # Get prediction of noise\n",
    "        noise_pred=model(xt,torch.as_tensor(i).unsqueeze.to(device))\n",
    "\n",
    "        # Use scheduler to get x0 and xt-1\n",
    "        xt,x0_pred=scheduler.sample_prev_timestep(xt,noise_pred,torch.as_tensor(i).to(device))\n",
    "\n",
    "\n",
    "        # Save x0\n",
    "        ims=torch.clamp(xt,-1.,1.).detach().cpu()\n",
    "        ims=(ims+1)/2\n",
    "        grid=make_grid(ims,nrow=train_config['num_grid_rows'])\n",
    "\n",
    "        img=torchvision.transforms.ToPILImage()(grid)\n",
    "        if not os.path.exists(os.path.join(train_config['task_name'],'samples')):\n",
    "            os.mkdir(os.path.join(train_config['task_name'],'samples'))\n",
    "\n",
    "        img.save(os.path.join(train_config['task_name'],'samples','x0_{}.png'.format(i)))\n",
    "\n",
    "        img.close()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    argparse=argParse()\n",
    "    parser=argparse.ArgumentParser(description='Argument for ddpm image generation')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
